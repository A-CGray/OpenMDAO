{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569d931",
   "metadata": {
    "tags": [
     "remove-input",
     "active-ipynb",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e8af7",
   "metadata": {},
   "source": [
    "(sec:openmdao_math)=\n",
    "# Math Library (`openmdao.math`)\n",
    "\n",
    "Certain functions are useful in a gradient-based optimization context, such as smooth activation functions or differentiable maximum/minimum functions.\n",
    "\n",
    "Rather than provide a component that forces a user to structure their system in a certain way and add more components than necessary, the `openmdao.math` package is intended to provide a universal source for _composable_ functions that users can use within their own components.\n",
    "\n",
    "Functions in `openmdao.math` are built using the [jax](https://github.com/google/jax).\n",
    "This allows users to develop components that use these functions, along with other code written with jax, and leverage capabilities of `jax` like automatic differentiation and just-in-time compilation.\n",
    "\n",
    "Many of these functions are focused on providing differentiable forms of strictly non-differentiable functions, such as step responses, absolute value, and minimums or maximums.\n",
    "Near regions where the nominal functions would have invalid derivatives, these functions are smooth but will not perfectly match their non-smooth counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86efbe58",
   "metadata": {},
   "source": [
    "## Available Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201b2ba",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.math.act_tanh\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fa1f1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openmdao.math as omm\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "fig.suptitle('Impact of different parameters on act_tanh')\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "mup001 = omm.act_tanh(x, mu=0.001, z=0.5, a=0, b=1)\n",
    "mup01 = omm.act_tanh(x, mu=0.01, z=0.5, a=0, b=1)\n",
    "mup1 = omm.act_tanh(x, mu=0.1, z=0.5, a=0, b=1)\n",
    "\n",
    "ax[0, 0].plot(x, mup001, label=r'$\\mu$ = 0.001')\n",
    "ax[0, 0].plot(x, mup01, label=r'$\\mu$ = 0.01')\n",
    "ax[0, 0].plot(x, mup1, label=r'$\\mu$ = 0.1')\n",
    "ax[0, 0].legend()\n",
    "ax[0, 0].grid()\n",
    "\n",
    "zp5 = omm.act_tanh(x, mu=0.01, z=0.5, a=0, b=1)\n",
    "zp4 = omm.act_tanh(x, mu=0.01, z=0.4, a=0, b=1)\n",
    "zp6 = omm.act_tanh(x, mu=0.01, z=0.6, a=0, b=1)\n",
    "\n",
    "ax[0, 1].plot(x, zp4, label=r'$z$ = 0.4')\n",
    "ax[0, 1].plot(x, zp5, label=r'$z$ = 0.5')\n",
    "ax[0, 1].plot(x, zp6, label=r'$z$ = 0.6')\n",
    "ax[0, 1].legend()\n",
    "ax[0, 1].grid()\n",
    "\n",
    "a0 = omm.act_tanh(x, mu=0.01, z=0.5, a=0, b=1)\n",
    "ap2 = omm.act_tanh(x, mu=0.01, z=0.5, a=0.2, b=1)\n",
    "ap4 = omm.act_tanh(x, mu=0.01, z=0.5, a=0.4, b=1)\n",
    "\n",
    "ax[1, 0].plot(x, a0, label=r'$a$ = 0.0')\n",
    "ax[1, 0].plot(x, ap2, label=r'$a$ = 0.2')\n",
    "ax[1, 0].plot(x, ap4, label=r'$a$ = 0.4')\n",
    "ax[1, 0].legend()\n",
    "ax[1, 0].grid()\n",
    "\n",
    "bp6 = omm.act_tanh(x, mu=0.01, z=0.5, a=0, b=.6)\n",
    "bp8 = omm.act_tanh(x, mu=0.01, z=0.5, a=0, b=.8)\n",
    "b1 = omm.act_tanh(x, mu=0.01, z=0.5, a=0, b=1)\n",
    "\n",
    "ax[1, 1].plot(x, bp6, label=r'$b$ = 0.6')\n",
    "ax[1, 1].plot(x, bp8, label=r'$b$ = 0.8')\n",
    "ax[1, 1].plot(x, b1, label=r'$b$ = 1.0')\n",
    "ax[1, 1].legend()\n",
    "ax[1, 1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b50cc",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.math.smooth_abs\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f5047",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on smooth_abs')\n",
    "x = np.linspace(-0.2, 0.2, 1000)\n",
    "\n",
    "mup001 = omm.smooth_abs(x, mu=0.001)\n",
    "mup01 = omm.smooth_abs(x, mu=0.01)\n",
    "mup1 = omm.smooth_abs(x, mu=0.1)\n",
    "\n",
    "ax.plot(x, mup001, label=r'$\\mu$ = 0.001')\n",
    "ax.plot(x, mup01, label=r'$\\mu$ = 0.01')\n",
    "ax.plot(x, mup1, label=r'$\\mu$ = 0.1')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268fc9f7",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.math.smooth_max\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cb8c2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on smooth_max of sin and cos')\n",
    "x = np.linspace(0.5, 1, 1000)\n",
    "\n",
    "sin = np.sin(x)\n",
    "cos = np.cos(x)\n",
    "\n",
    "mup001 = omm.smooth_max(sin, cos, mu=0.001)\n",
    "mup01 = omm.smooth_max(sin, cos, mu=0.01)\n",
    "mup1 = omm.smooth_max(sin, cos, mu=0.1)\n",
    "\n",
    "ax.plot(x, sin, '--', label=r'$\\sin{x}$')\n",
    "ax.plot(x, cos, '--', label=r'$\\cos{x}$')\n",
    "ax.plot(x, mup01, label=r'$\\mu$ = 0.01')\n",
    "ax.plot(x, mup1, label=r'$\\mu$ = 0.1')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3f93a",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.math.smooth_min\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322fd31",
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on smooth_min of sin and cos')\n",
    "x = np.linspace(0.5, 1, 1000)\n",
    "\n",
    "sin = np.sin(x)\n",
    "cos = np.cos(x)\n",
    "\n",
    "mup001 = omm.smooth_min(sin, cos, mu=0.001)\n",
    "mup01 = omm.smooth_min(sin, cos, mu=0.01)\n",
    "mup1 = omm.smooth_min(sin, cos, mu=0.1)\n",
    "\n",
    "ax.plot(x, sin, '--', label=r'$\\sin{x}$')\n",
    "ax.plot(x, cos, '--', label=r'$\\cos{x}$')\n",
    "ax.plot(x, mup01, label=r'$\\mu$ = 0.01')\n",
    "ax.plot(x, mup1, label=r'$\\mu$ = 0.1')\n",
    "ax.legend(ncol=2)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828da93",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    "    .. autofunction:: openmdao.math.ks_max\n",
    "        :noindex:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24ddab",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from openmdao.math import ks_max\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on ks_max')\n",
    "y = np.random.random(100)\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "rho1 = ks_max(y, rho=10.)\n",
    "rho10 = ks_max(y, rho=100.)\n",
    "rho100 = ks_max(y, rho=1000.)\n",
    "\n",
    "ax.plot(x, y, '.', label='y')\n",
    "ax.plot(x, rho1 * np.ones_like(x), label='ks_max(y, rho=10)')\n",
    "ax.plot(x, rho10 * np.ones_like(x), label='ks_max(y, rho=100)')\n",
    "ax.legend(ncol=1)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea9f69",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from openmdao.math import ks_min\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "fig.suptitle('Impact of different parameters on ks_max')\n",
    "y = np.random.random(100) + 5\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "rho1 = ks_min(y, rho=10.)\n",
    "rho10 = ks_min(y, rho=100.)\n",
    "rho100 = ks_min(y, rho=1000.)\n",
    "\n",
    "ax.plot(x, y, '.', label='y')\n",
    "ax.plot(x, rho1 * np.ones_like(x), label='ks_min(y, rho=10)')\n",
    "ax.plot(x, rho10 * np.ones_like(x), label='ks_min(y, rho=100)')\n",
    "ax.legend(ncol=1)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed36a21",
   "metadata": {},
   "source": [
    "## Getting derivatives from jax-composed functions\n",
    "\n",
    "If the user write a function that is composed entirely using jax-based functions (from `jax.numpy`, etc.), then `jax` will in most cases be able to provide derivatives of those functions automatically.\n",
    "\n",
    "The library has several ways of doing this and the best approach will likely depend on the specific use-case hand.\n",
    "Rather than provide a component to wrap a `jax` function and provide derivatives automatically, consider the following example as a template for how to utilize `jax` in combination with OpenMDAO components.\n",
    "\n",
    "The following component wraps the `act_tanh` function in `openmdao.math`. In this case we've assumed values for `$\\mu$`, `a`, and `b`, and will only be passing a vector input `x` as well as the activation value `z`.\n",
    "The output `h` will have a value of approximately `1.0` in those indices of `x` where `x > z`. The value of `h` will be approximately `0.0` in those indices of `x` where `x < z`, and there will be some smooth transition between them. The smoothness of this transition is goverened by `$\\mu$`, with smaller values yielding behavior closer to a true step (at the expence of more abrupt changes in the derivatives near `z`.\n",
    "\n",
    "### compute_primal\n",
    "\n",
    "In this particular instance, we declare a method of the component named `compute_primal`.\n",
    "That function name is not special to OpenMDAO and the user could call this function whatever they choose so long as it doesn't interfere with some pre-existing component method name.\n",
    "In addition to the `self` argument, `compute_primal` takes positional arguments to make it compatible with `jax`.\n",
    "We also wrap the method with the `jax.jit` decorator (and use `static_argnums` to inform it that the first argument (`self`) is not relevant to `jax`.\n",
    "\n",
    "### compute\n",
    "Compute in this case is just a matter of passing the values of the inputs to `compute_primal` and populating the outputs with the results.\n",
    "\n",
    "###  compute_partials\n",
    "\n",
    "Computing the partial derivatives across the component is done by passing the inputs to a separate method. Since there are multiple ways of computing the partials with `jax`, this example has four different `_compute_partials_xxx` methods, though only one is used.\n",
    "\n",
    "Again, these method names are not special and are only used in the context of this example.\n",
    "\n",
    "### _compute_partials_jacfwd\n",
    "\n",
    "This uses the `jax.jacfwd` method to compuite the partial derivatives of the calculation with a forward differentiation approach.\n",
    "This approach should be one of the faster options when there are comparatively few inputs versus outputs.\n",
    "\n",
    "Note that because we know that the sparsity structure of the jacobian for the output `h` wrt `x` will be diagonal, we extract the diagonal from the derivative that `jax` returns. We also declared this sparsity structure in the corresponding `declare_partials` call.\n",
    "This pattern is common in vectorized functions but ultimately it's up to the user to know the sparsity structure when they implement the component.\n",
    "\n",
    "### _compute_partials_jacrev\n",
    "\n",
    "This is similar to the previous approach except `jax.jacrev` is used.\n",
    "This function still returns dense jacobians and so the diagonal is extracted from the derivative of `h` with respect to `x`.\n",
    "\n",
    "Reverse differentiation should be faster when the number of outputs of a function is significantly fewer than the number of inputs, such as in reduction operations.\n",
    "\n",
    "### _compute_partials_jvp\n",
    "\n",
    "Because we know the sparisty structure will be diagonal for the derivative of `h` with respect to `x`, we can use the jacobian vector product method provided by `jax` to extract the diagonal elements only, rather than computing the full matrix and extracting the diagonal.\n",
    "\n",
    "Because there are two arguments we call `jax.jvp` two times, once with the tangents of `x` populated with ones and the tangents of `z` populated with zeros (to compute the partials with respect to `x`), and once for the other way around.\n",
    "\n",
    "### Which approach to use?\n",
    "\n",
    "In practice, it's going to be a matter of the user profiling their code to see which of these approaches is fastest.\n",
    "For the example below, some testing indicated that `jvp` was _slightly_ faster.\n",
    "\n",
    "An analytic implementation of the derivatives of `act_tanh` was _slightly_ faster than all of these, but only by a few percent at most.\n",
    "The ability to provide accurate derivatives with little effort vs. hand differentiation at a nearly negligible loss in performance is an attractive proposition for the use of `jax` in OpenMDAO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from functools import partial\n",
    "    import numpy as np\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import openmdao.api as om\n",
    "\n",
    "\n",
    "    class ActTanhComp(om.ExplicitComponent):\n",
    "\n",
    "        def initialize(self):\n",
    "            self.options.declare('vec_size', types=int)\n",
    "\n",
    "        def setup(self):\n",
    "            N = self.options['vec_size']\n",
    "            self.add_input('x', shape=(N,))\n",
    "            self.add_input('z', shape=(1,))\n",
    "            self.add_output('h', shape=(N,))\n",
    "\n",
    "            ar = np.arange(N, dtype=int)\n",
    "\n",
    "            self.declare_partials(of='h', wrt='x', rows=ar, cols=ar)\n",
    "            self.declare_partials(of='h', wrt='z')\n",
    "\n",
    "        @partial(jax.jit, static_argnums=(0,))\n",
    "        def compute_primal(self, x, z):\n",
    "            \"\"\"\n",
    "            This is where the jax implementation belongs.\n",
    "            \"\"\"\n",
    "            return act_tanh(x, 0.01, z, 0.0, 1.0)\n",
    "\n",
    "        @partial(jax.jit, static_argnums=(0,))\n",
    "        def _compute_partials_jacfwd(self, x, z):\n",
    "            deriv_func = jax.jacfwd(self.compute_primal, argnums=[0, 1])\n",
    "            dx, dz = deriv_func(x, z)\n",
    "            return jnp.diagonal(dx), dz\n",
    "\n",
    "        @partial(jax.jit, static_argnums=(0,))\n",
    "        def _compute_partials_jacrev(self, x, z):\n",
    "            deriv_func = jax.jacrev(self.compute_primal, argnums=[0, 1])\n",
    "            dx, dz = deriv_func(x, z)\n",
    "            return jnp.diagonal(dx), dz\n",
    "\n",
    "        @partial(jax.jit, static_argnums=(0,))\n",
    "        def _compute_partials_jvp(self, x, z):\n",
    "            dx = jax.jvp(self.compute_primal,\n",
    "                         primals=(x, z),\n",
    "                         tangents=(jnp.ones_like(x), jnp.zeros_like(z)))[1]\n",
    "\n",
    "            dz = jax.jvp(self.compute_primal,\n",
    "                         primals=(x, z),\n",
    "                         tangents=(jnp.zeros_like(x), jnp.ones_like(z)))[1]\n",
    "\n",
    "            return dx, dz\n",
    "\n",
    "        def compute(self, inputs, outputs, discrete_inputs=None, discrete_outputs=None):\n",
    "            outputs['h'] = self.compute_primal(*inputs.values())\n",
    "\n",
    "        def compute_partials(self, inputs, partials, discrete_inputs=None):\n",
    "            dx, dz = self._compute_partials_jvp(*inputs.values())\n",
    "\n",
    "            partials['h', 'x'] = dx\n",
    "            partials['h', 'z'] = dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1302976",
   "metadata": {},
   "source": [
    "## Example Use-Case - Differentiable Counting\n",
    "\n",
    "Suppose we have some array of data and we want a count of the number of elements in the array that are greater than or equal to some given value.\n",
    "\n",
    "We can use the `act_tanh` function and set it to return 0 for values less than our threshold, and 1 for valeus greater than our threshold.\n",
    "\n",
    "Summing the result of _this_ function will then give us the count.\n",
    "It will be approximate in that there is some inaccuracy where the activation function is smoothed, but it will be differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc845ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "import openmdao.api as om\n",
    "from openmdao.math import act_tanh\n",
    "\n",
    "\n",
    "class CountingComp(om.ExplicitComponent):\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.options.declare('vec_size', types=(int,))\n",
    "        self.options.declare('threshold', types=(float,), default=0.0)\n",
    "        self.options.declare('mu', types=(float,), default=0.01)\n",
    "    \n",
    "    def setup(self):\n",
    "        n = self.options['vec_size']\n",
    "        self.add_input('x', shape=(n,))\n",
    "        self.add_output('count', shape=(1,))\n",
    "        \n",
    "        # The partials are a dense row in this case (1 row x N inputs)\n",
    "        # There is no need to specify a sparsity pattern.\n",
    "        self.declare_partials(of='count', wrt='x')\n",
    "\n",
    "#     @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jacfwd(self, x):\n",
    "        deriv_func = jax.jacfwd(self.compute_primal, argnums=[0])\n",
    "        dx, = deriv_func(x)\n",
    "        return dx\n",
    "    \n",
    "#     @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jacrev(self, x):\n",
    "        deriv_func = jax.jacrev(self.compute_primal, argnums=[0])\n",
    "        # Always returns a tuple\n",
    "        dx, = deriv_func(x)\n",
    "        return dx\n",
    "\n",
    "#     @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jvp(self, x):\n",
    "        # Note that JVP is a poor choice here, since the jacobian is a row vector!\n",
    "        # We have to call it once with each individual element in x set to 1.0\n",
    "        # while all the others are zero in order to get a correct result!\n",
    "        \n",
    "        # jvp always returns the primal and the jvp\n",
    "        # This will give incorrect results! There is \"cross-talk\" amongs the different\n",
    "        # indices in the tangents.\n",
    "        _, dx = jax.jvp(self.compute_primal,\n",
    "                        primals=(x,),\n",
    "                        tangents=(jnp.ones_like(x),))\n",
    "        return dx\n",
    "\n",
    "    # TODO: how to properly use vjp?\n",
    "    #     @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_vjp(self, x):\n",
    "        # Note that JVP is a poor choice here, since the jacobian is a row vector!\n",
    "        # We have to call it once with each individual element in x set to 1.0\n",
    "        # while all the others are zero in order to get a correct result!\n",
    "        \n",
    "        # jvp always returns the primal and the jvp\n",
    "        # This will give incorrect results! There is \"cross-talk\" amongs the different\n",
    "        # indices in the tangents.\n",
    "        _, vjp_fun = jax.vjp(self.compute_primal, x)\n",
    "        dx = vjp_fun(self.compute_primal(x))\n",
    "        return dx\n",
    " \n",
    "    # TODO: how to use jax.jit and get self.<attribute> into the function here? Wrapped function?\n",
    "#     @partial(jax.jit, static_argnums=(0,))\n",
    "    def compute_primal(self, x):\n",
    "        mu = self.options['mu']\n",
    "        z = self.options['threshold']\n",
    "        return jnp.sum(act_tanh(x, mu, z, 0.0, 1.0))\n",
    "    \n",
    "    def compute(self, inputs, outputs):\n",
    "        z = self.options['threshold']\n",
    "        x = inputs['x']\n",
    "        mu = self.options['mu']\n",
    "        \n",
    "        outputs['count'] = self.compute_primal(*inputs.values())\n",
    "        \n",
    "    def compute_partials(self, inputs, partials):\n",
    "        dx = self._compute_partials_jacrev(*inputs.values())\n",
    "\n",
    "        partials['count', 'x'] = dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d19c963",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "p = om.Problem()\n",
    "p.model.add_subsystem('counter',\n",
    "                      CountingComp(vec_size=N,threshold=0.5, mu=0.01),\n",
    "                      promotes_inputs=['x'], promotes_outputs=['count'])\n",
    "p.setup(force_alloc_complex=True)\n",
    "p.set_val('x', np.linspace(0, 1, N))\n",
    "p.run_model()\n",
    "p.model.list_inputs(print_arrays=True)\n",
    "p.model.list_outputs(print_arrays=True)\n",
    "\n",
    "with np.printoptions(linewidth=1024):\n",
    "    p.check_partials(method='cs', compact_print=False);"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
